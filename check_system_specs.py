#!/usr/bin/env python3

import psutil
import platform
import os

def check_system_specs():
    """Check system specifications and recommend Ollama models"""
    
    print("üñ•Ô∏è  System Specifications Check")
    print("=" * 50)
    
    # RAM
    ram_gb = psutil.virtual_memory().total / (1024**3)
    print(f"üìä RAM: {ram_gb:.1f} GB")
    
    # CPU
    cpu_count = psutil.cpu_count()
    cpu_freq = psutil.cpu_freq()
    print(f"üñ•Ô∏è  CPU: {cpu_count} cores")
    if cpu_freq:
        print(f"    Frequency: {cpu_freq.current:.0f} MHz")
    
    # Storage
    disk = psutil.disk_usage('/')
    disk_gb = disk.free / (1024**3)
    print(f"üíæ Free Storage: {disk_gb:.1f} GB")
    
    # OS
    print(f"ü™ü OS: {platform.system()} {platform.release()}")
    
    print("\nüéØ Ollama Model Recommendations:")
    print("=" * 50)
    
    # Recommendations based on RAM
    if ram_gb >= 32:
        print("‚úÖ HIGH-END SYSTEM - All models recommended:")
        print("   ‚Ä¢ Llama2 13B (Best quality)")
        print("   ‚Ä¢ CodeLlama 13B (Technical topics)")
        print("   ‚Ä¢ Mistral 7B (Fast and good)")
        print("   ‚Ä¢ Phi-2 (Lightning fast)")
        
    elif ram_gb >= 16:
        print("‚úÖ MID-RANGE SYSTEM - Most models work well:")
        print("   ‚Ä¢ Llama2 7B (Good quality)")
        print("   ‚Ä¢ CodeLlama 7B (Technical topics)")
        print("   ‚Ä¢ Mistral 7B (Fast and good)")
        print("   ‚Ä¢ Phi-2 (Lightning fast)")
        
    elif ram_gb >= 8:
        print("‚úÖ STANDARD SYSTEM - Smaller models recommended:")
        print("   ‚Ä¢ Phi-2 (Best choice - fast and efficient)")
        print("   ‚Ä¢ Mistral 7B (Good quality, moderate speed)")
        print("   ‚Ä¢ Llama2 7B (Good quality, slower)")
        
    else:
        print("‚ö†Ô∏è  LIMITED SYSTEM - Consider these options:")
        print("   ‚Ä¢ Phi-2 (Only recommended option)")
        print("   ‚Ä¢ Or use template-based responses (no LLM)")
        print("   ‚Ä¢ Or consider cloud APIs (Gemini, OpenAI)")
    
    print("\nüí° Performance Tips:")
    print("=" * 50)
    
    if ram_gb < 16:
        print("‚Ä¢ Close other applications when using Ollama")
        print("‚Ä¢ Start with Phi-2 model for best performance")
        print("‚Ä¢ Consider upgrading RAM if possible")
    
    if disk_gb < 10:
        print("‚Ä¢ Free up disk space before installing models")
        print("‚Ä¢ Models require 2-8GB each")
    
    print("\nüöÄ Quick Start Commands:")
    print("=" * 50)
    print("1. Install Ollama: https://ollama.ai")
    print("2. Start service: ollama serve")
    print("3. Download model: ollama pull phi-2")
    print("4. Test: ollama run phi-2 'Hello!'")
    
    print("\nüìä Expected Performance:")
    print("=" * 50)
    
    if ram_gb >= 16:
        print("‚Ä¢ Response time: 2-5 seconds")
        print("‚Ä¢ Quality: Excellent")
        print("‚Ä¢ Can run multiple models")
    elif ram_gb >= 8:
        print("‚Ä¢ Response time: 3-8 seconds")
        print("‚Ä¢ Quality: Good")
        print("‚Ä¢ Best with Phi-2 or Mistral")
    else:
        print("‚Ä¢ Response time: 5-15 seconds")
        print("‚Ä¢ Quality: Acceptable")
        print("‚Ä¢ Only Phi-2 recommended")

if __name__ == "__main__":
    try:
        check_system_specs()
    except Exception as e:
        print(f"Error checking system specs: {e}")
        print("\nüí° Manual Check:")
        print("‚Ä¢ Right-click Start ‚Üí System")
        print("‚Ä¢ Look for 'Installed RAM'")
        print("‚Ä¢ 8GB+ = Good for Ollama")
        print("‚Ä¢ 16GB+ = Excellent for Ollama") 